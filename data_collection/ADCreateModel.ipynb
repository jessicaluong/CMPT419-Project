{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ADUtils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether you already have data stored in a npy file\n",
    "load_from_npy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads data if it is not already stored in a npy file. \n",
    "if not load_from_npy:\n",
    "\n",
    "    sequences, labels = [], []\n",
    "    skip_sequence = False\n",
    "    for action in actions:\n",
    "        action_path = os.path.join(DATA_PATH, action)\n",
    "        if not os.path.exists(action_path):\n",
    "            continue\n",
    "        for sequence in np.array(os.listdir(action_path)).astype(int):\n",
    "            window = []\n",
    "            skip_sequence = False\n",
    "            for frame_num in range(sequence_length):\n",
    "                sequence_path = os.path.join(action_path, str(sequence))\n",
    "                # 1. make sure path exists\n",
    "                if not os.path.exists(sequence_path):\n",
    "                    print(\"1\")\n",
    "                    skip_sequence = True\n",
    "                    break\n",
    "                # 2. make sure none are empty\n",
    "                try:\n",
    "                    res = np.load(os.path.join(sequence_path, \"{}.npy\".format(frame_num)))\n",
    "                    window.append(res)\n",
    "                except:\n",
    "                    print(f\"Cannot read {sequence_path} number {frame_num}\")\n",
    "                    print(\"2\")\n",
    "                    skip_sequence = True\n",
    "                    break\n",
    "            if not skip_sequence:\n",
    "                # 3. make sure there is at least 30 frames\n",
    "                if len(window) == 30:\n",
    "                    sequences.append(window)\n",
    "                    labels.append(label_map[action])\n",
    "                    continue\n",
    "                print(\"3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Input Data\n",
    "\n",
    "The tuple from the next command shows the dimension of our data. The details of the data is as follows. \n",
    "\n",
    "The first tuple displays the number of samples we collected. We collected samples for seven actions: 'raise_hand', 'thumbs_up', 'thumbs_down', 'cheer', 'cross_arms', 'clap', 'neutral'. For each of these actions, we captured variations of them to ensure we're covering variability for each action. This list describes the variation and number of samples for each:  \n",
    "- raise_hand: 30 left hand only (15 hand on screen and 15 off), 30 right hand only (15 on screen and 15 off)\n",
    "- thumbs_up: 30 left hand only, 30 right hand only, 30 both hands\n",
    "- thumbs_down: 30 left hand only, 30 right hand only, 30 both hands \n",
    "- cheer: 30 hands below head, 30 hands above head \n",
    "- cross_arms: 30 no hands, 30 right hand only, 30 left hand only, 30 both hands\n",
    "- clap: 30 right hand over left, 30 left hand over right, 30 both hands vertical to the screen\n",
    "- neutral: 30 no hands \n",
    "\n",
    "The second element in the tuple shows that each sample has 30 frames. \n",
    "\n",
    "The last element in the tuple shows the number of data points per frame. These were extracted from the MediaPipe Holistic model. The features we are using include 33 pose landmarks (each of which have coordinates x, y, z, and visibility) and 21 hand landmarks (each have coordinates x, y, z). Moreover, we are collecing landmarks for both the left and the right hands. In total this is 258 coordinates (33 * 4 + 21 * 3 + 21 * 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(sequences).shape)\n",
    "# number of samples, number of frames per sample, number of data points per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_numpy = False\n",
    "if save_to_numpy and not load_from_numpy:\n",
    "    np.save('action_detect_X.npy', sequences)\n",
    "    np.save('action_detect_y.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_npy:\n",
    "    X = np.array(sequences)\n",
    "else:\n",
    "    X = np.load('action_detect.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_from_npy:\n",
    "    y = to_categorical(labels).astype(int)\n",
    "else:\n",
    "    y = np.load('action_detect_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "# might remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Architecture\n",
    "\n",
    "We used multiple LSTM layers stacked together to allow the model to learn higher-level temporal representations. Each layer potentially captures different aspects of temporal dependencies in the data. The ReLU activation function in the dense layers introduces non-linearity into the model, allowing it to learn more complex patterns in the data. The final dense layer uses a softmax activation for classification. We found that this amount of layers allowed the model to learn complex relationships in the data without overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Input(shape=(30,258)))\n",
    "model.add(Input(shape=X_train.shape[-2:]))\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    model.save('action.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Initial Model with 5-Fold-Cross-Validation\n",
    "\n",
    "We used 5-fold-cross-validation to check if our model was overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)  \n",
    "labels = np.array(labels)  \n",
    "\n",
    "# Split the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # Validation is 20% of the original\n",
    "\n",
    "if len(y_train.shape) == 1:  \n",
    "    y_train = to_categorical(y_train, num_classes=actions.shape[0])\n",
    "    y_val = to_categorical(y_val, num_classes=actions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_val = to_categorical(y_train_val, num_classes=np.unique(y_train_val).size)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_val):\n",
    "    # Split data\n",
    "    X_train, X_val = X_train_val[train_index], X_train_val[val_index]\n",
    "    y_train, y_val = y_train_val[train_index], y_train_val[val_index]\n",
    "    # \n",
    "    model = Sequential()\n",
    "    # model.add(Input(shape=(30,258)))\n",
    "    model.add(Input(shape=X_train.shape[-2:]))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "    # Evaluate model\n",
    "    scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "    print(f\"Score for fold {fold_no}: Accuracy of {scores[1]*100:.2f}%\")\n",
    "    accuracies.append(scores[1] * 100)\n",
    "    fold_no += 1\n",
    "\n",
    "average_score = np.mean(accuracies)\n",
    "print(f\"Average Score: {average_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Monitoring to Find Number of Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lets you see which number of epochs may be best for specified batch size. \n",
    "\n",
    "As shown by 5-cross-fold-validation, our initial implementation was good. We wanted to see if it could be further improved with different number of epochs and batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for early stopping and saving the best model\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentionally remove statistics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if 5-fold-cross-validation was used\n",
    "# y_true = y_test.tolist() \n",
    "# y_pred = np.argmax(y_prob, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this out if 5-fold-cross-validation was used\n",
    "y_true = np.argmax(y_test, axis=1).tolist()\n",
    "y_pred = np.argmax(y_prob, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=actions)\n",
    "disp.plot(cmap='GnBu', xticks_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "if save_model:\n",
    "    model = load_model('action.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the colors\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (245,117,16), (117,245,16), (16,117,245), (245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "threshold = 0.5\n",
    "\n",
    "# if you want to use video as testing\n",
    "# video = '../data/test/thumbs_up/thumbs_up9.mp4'\n",
    "# cap = cv2.VideoCapture(video)\n",
    "\n",
    "# if you want a bigger video at the cost of performance\n",
    "# cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        word = ''\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            word = actions[np.argmax(res)]\n",
    "                \n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "\n",
    "        cv2.putText(image, word, (3,30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
